// LLM í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬
export interface LLMConfig {
  provider: 'openai' | 'claude' | 'gemini' | 'runware';
  model: string;
  apiKey: string;
  style?: string;
}

export interface LLMResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMTool {
  name: string;
  description: string;
  parameters: any;
}

export interface LLMGenerateOptions {
  messages: LLMMessage[];
  tools?: LLMTool[];
  maxIterations?: number;
}

export abstract class BaseLLMClient {
  protected config: LLMConfig;

  constructor(config: LLMConfig) {
    this.config = config;
  }

  abstract generateText(messages: LLMMessage[], options?: { tools?: LLMTool[] }): Promise<LLMResponse>;
  abstract generateImage(prompt: string, options?: { quality?: 'low' | 'medium' | 'high'; size?: '512x768' | '768x512' | '1024x1024' | '1024x1536' | '1536x1024' }): Promise<string>; // ì´ë¯¸ì§€ URL ë°˜í™˜
}

export class OpenAIClient extends BaseLLMClient {
  async generateText(messages: LLMMessage[], options?: { tools?: LLMTool[] }): Promise<LLMResponse> {
    const maxRetries = 2;
    
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        console.log(`ğŸ”µ OpenAI ${this.config.model} í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘ (${attempt}/${maxRetries})`);
        
        const response = await fetch('https://api.openai.com/v1/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${this.config.apiKey}`
          },
          body: JSON.stringify({
            model: this.config.model,
            messages: messages.map(msg => ({
              role: msg.role,
              content: msg.content
            })),
            temperature: 0.7,
            max_tokens: 2000
          })
        });

        if (!response.ok) {
          const errorText = await response.text();
          console.error(`âŒ OpenAI ì˜¤ë¥˜ ì‘ë‹µ (${attempt}/${maxRetries}):`, errorText);
          
          if (attempt === maxRetries) {
            throw new Error(`OpenAI API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
          }
          
          // ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸° (500ms * attempt)
          await new Promise(resolve => setTimeout(resolve, 500 * attempt));
          continue;
        }

        const data = await response.json();
        console.log(`âœ… OpenAI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ`);
        
        return {
          content: data.choices[0]?.message?.content || '',
          usage: {
            promptTokens: data.usage?.prompt_tokens || 0,
            completionTokens: data.usage?.completion_tokens || 0,
            totalTokens: data.usage?.total_tokens || 0
          }
        };
        
      } catch (error) {
        console.error(`OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (${attempt}/${maxRetries}):`, error);
        
        if (attempt === maxRetries) {
          throw error;
        }
        
        // ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸° (500ms * attempt)
        await new Promise(resolve => setTimeout(resolve, 500 * attempt));
      }
    }
    
    throw new Error('OpenAI í…ìŠ¤íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
  }

  async generateImage(prompt: string, options?: { quality?: 'low' | 'medium' | 'high'; size?: '512x768' | '768x512' | '1024x1024' | '1024x1536' | '1536x1024' }): Promise<string> {
    const maxRetries = 2;
    
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        console.log(`ğŸ”µ OpenAI gpt-image-1 ì´ë¯¸ì§€ ìƒì„± ì‹œì‘ (${attempt}/${maxRetries})`);
        
        // OpenAIëŠ” ì œí•œëœ í•´ìƒë„ë§Œ ì§€ì›
        const requestSize = options?.size || '1024x1024';
        
        const response = await fetch('https://api.openai.com/v1/images/generations', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${this.config.apiKey}`
          },
          body: JSON.stringify({
            model: 'gpt-image-1',
            prompt: prompt,
            size: requestSize, // '1024x1024', '1024x1536', '1536x1024' ì¤‘ ì„ íƒ
            n: 1
            // gpt-image-1ì€ í•­ìƒ base64ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ response_format ë¶ˆí•„ìš”
            // quality íŒŒë¼ë¯¸í„°ë„ gpt-image-1ì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ
          })
        });

        console.log(`ğŸ“Š OpenAI ì‘ë‹µ ìƒíƒœ: ${response.status}`);

        if (!response.ok) {
          const errorText = await response.text();
          console.error(`âŒ OpenAI ì˜¤ë¥˜ ì‘ë‹µ (${attempt}/${maxRetries}):`, errorText);
          
          if (attempt === maxRetries) {
            throw new Error(`OpenAI Image API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
          }
          
          // ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸° (500ms * attempt)
          await new Promise(resolve => setTimeout(resolve, 500 * attempt));
          continue;
        }

        const data = await response.json();
        console.log(`âœ… OpenAI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ`);
        
        // gpt-image-1ì€ base64 í˜•íƒœë¡œ ë°˜í™˜
        const base64Image = data.data?.[0]?.b64_json;
        if (base64Image) {
          return `data:image/png;base64,${base64Image}`;
        } else {
          console.error('OpenAI ì‘ë‹µ êµ¬ì¡°:', JSON.stringify(data, null, 2));
          
          if (attempt === maxRetries) {
            throw new Error('OpenAIì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.');
          }
          
          // ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
          await new Promise(resolve => setTimeout(resolve, 500 * attempt));
          continue;
        }
        
      } catch (error) {
        console.error(`OpenAI Image API í˜¸ì¶œ ì‹¤íŒ¨ (${attempt}/${maxRetries}):`, error);
        
        if (attempt === maxRetries) {
          throw error;
        }
        
        // ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸° (500ms * attempt)
        await new Promise(resolve => setTimeout(resolve, 500 * attempt));
      }
    }
    
    throw new Error('OpenAI ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
  }
}

export class ClaudeClient extends BaseLLMClient {
  async generateText(messages: LLMMessage[], options?: { tools?: LLMTool[] }): Promise<LLMResponse> {
    const maxRetries = 2;
    
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        console.log(`ğŸŸ£ Claude ${this.config.model} í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘ (${attempt}/${maxRetries})`);
        
        const response = await fetch('https://api.anthropic.com/v1/messages', {
          method: 'POST',
          headers: {
            'x-api-key': this.config.apiKey,
            'anthropic-version': '2023-06-01',
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            model: this.config.model,
            max_tokens: 2000,
            messages: messages.map(msg => ({
              role: msg.role === 'system' ? 'user' : msg.role,
              content: msg.role === 'system' ? `System: ${msg.content}` : msg.content
            }))
          })
        });

        if (!response.ok) {
          const errorText = await response.text();
          console.error(`âŒ Claude ì˜¤ë¥˜ ì‘ë‹µ (${attempt}/${maxRetries}):`, errorText);
          
          if (attempt === maxRetries) {
            throw new Error(`Claude API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
          }
          
          await new Promise(resolve => setTimeout(resolve, 500 * attempt));
          continue;
        }

        const data = await response.json();
        console.log(`âœ… Claude ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ`);
        
        return {
          content: data.content[0]?.text || '',
          usage: {
            promptTokens: data.usage?.input_tokens || 0,
            completionTokens: data.usage?.output_tokens || 0,
            totalTokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0)
          }
        };
        
      } catch (error) {
        console.error(`Claude API í˜¸ì¶œ ì‹¤íŒ¨ (${attempt}/${maxRetries}):`, error);
        
        if (attempt === maxRetries) {
          throw error;
        }
        
        await new Promise(resolve => setTimeout(resolve, 500 * attempt));
      }
    }
    
    throw new Error('Claude í…ìŠ¤íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
  }

  async generateImage(prompt: string, options?: { quality?: 'low' | 'medium' | 'high'; size?: '512x768' | '768x512' | '1024x1024' | '1024x1536' | '1536x1024' }): Promise<string> {
    throw new Error('ClaudeëŠ” ì´ë¯¸ì§€ ìƒì„±ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
  }
}

export class GeminiClient extends BaseLLMClient {
  async generateText(messages: LLMMessage[], options?: { tools?: LLMTool[] }): Promise<LLMResponse> {
    const maxRetries = 2;
    
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        console.log(`ğŸŸ¡ Gemini ${this.config.model} í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘ (${attempt}/${maxRetries})`);
        
        // ë©”ì‹œì§€ë¥¼ Gemini í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        let textContent = '';
        for (const message of messages) {
          if (message.role === 'system') {
            textContent += `System: ${message.content}\n\n`;
          } else if (message.role === 'user') {
            textContent += `User: ${message.content}`;
          }
        }

        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/${this.config.model}:generateContent?key=${this.config.apiKey}`,
          {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              contents: [{
                parts: [{
                  text: textContent
                }]
              }],
              generationConfig: {
                maxOutputTokens: 2000,
                temperature: 0.7
              }
            })
          }
        );

        if (!response.ok) {
          const errorText = await response.text();
          console.error(`âŒ Gemini ì˜¤ë¥˜ ì‘ë‹µ (${attempt}/${maxRetries}):`, errorText);
          
          if (attempt === maxRetries) {
            throw new Error(`Gemini API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
          }
          
          await new Promise(resolve => setTimeout(resolve, 500 * attempt));
          continue;
        }

        const data = await response.json();
        console.log(`âœ… Gemini ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ`);
        
        return {
          content: data.candidates[0]?.content?.parts[0]?.text || '',
          usage: {
            promptTokens: data.usageMetadata?.promptTokenCount || 0,
            completionTokens: data.usageMetadata?.candidatesTokenCount || 0,
            totalTokens: data.usageMetadata?.totalTokenCount || 0
          }
        };
        
      } catch (error) {
        console.error(`Gemini API í˜¸ì¶œ ì‹¤íŒ¨ (${attempt}/${maxRetries}):`, error);
        
        if (attempt === maxRetries) {
          throw error;
        }
        
        await new Promise(resolve => setTimeout(resolve, 500 * attempt));
      }
    }
    
    throw new Error('Gemini í…ìŠ¤íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
  }

  async generateImage(prompt: string, options?: { quality?: 'low' | 'medium' | 'high'; size?: '512x768' | '768x512' | '1024x1024' | '1024x1536' | '1536x1024' }): Promise<string> {
    const maxRetries = 2;
    
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        console.log(`ğŸŸ¡ Gemini 2.5 Flash Image ì´ë¯¸ì§€ ìƒì„± ì‹œì‘ (${attempt}/${maxRetries})`);
        
        // Gemini 2.5 Flash Image Preview ëª¨ë¸ ì‚¬ìš© (2025ë…„ 8ì›” ì¶œì‹œ)
        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image-preview:generateContent?key=${this.config.apiKey}`,
          {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              contents: [{
                parts: [{
                  text: prompt // ì§ì ‘ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (Create an image: ì ‘ë‘ì–´ ë¶ˆí•„ìš”)
                }]
              }]
            })
          }
        );

        console.log(`ğŸ“Š Gemini ì‘ë‹µ ìƒíƒœ: ${response.status}`);

        if (!response.ok) {
          const errorText = await response.text();
          console.error(`âŒ Gemini ì˜¤ë¥˜ ì‘ë‹µ (${attempt}/${maxRetries}):`, errorText);
          
          if (attempt === maxRetries) {
            throw new Error(`Gemini Image API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
          }
          
          // ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸° (500ms * attempt)
          await new Promise(resolve => setTimeout(resolve, 500 * attempt));
          continue;
        }

        const data = await response.json();
        console.log(`âœ… Gemini ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ`);
        
        // Gemini 2.5 Flash Imageì˜ ì‹¤ì œ ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¥¸ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
        const parts = data.candidates?.[0]?.content?.parts;
        let imageData = null;
        
        if (parts && Array.isArray(parts)) {
          // parts ë°°ì—´ì—ì„œ inlineDataê°€ ìˆëŠ” ìš”ì†Œ ì°¾ê¸°
          for (const part of parts) {
            if (part.inlineData && part.inlineData.data) {
              imageData = part.inlineData.data;
              break;
            }
          }
        }
        
        if (imageData) {
          console.log('âœ… Gemini ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ');
          // Base64 ë°ì´í„°ë¥¼ data URLë¡œ ë³€í™˜
          return `data:image/png;base64,${imageData}`;
        } else {
          console.error('Gemini ì‘ë‹µ êµ¬ì¡°:', JSON.stringify(data, null, 2));
          
          if (attempt === maxRetries) {
            throw new Error('Geminiì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
          }
          
          // ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
          await new Promise(resolve => setTimeout(resolve, 500 * attempt));
          continue;
        }
        
      } catch (error) {
        console.error(`Gemini Image API í˜¸ì¶œ ì‹¤íŒ¨ (${attempt}/${maxRetries}):`, error);
        
        if (attempt === maxRetries) {
          throw error;
        }
        
        // ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸° (500ms * attempt)
        await new Promise(resolve => setTimeout(resolve, 500 * attempt));
      }
    }
    
    throw new Error('Gemini ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
  }
}

export class RunwareClient extends BaseLLMClient {
  async generateText(messages: LLMMessage[], options?: { tools?: LLMTool[] }): Promise<LLMResponse> {
    throw new Error('RunwareëŠ” í…ìŠ¤íŠ¸ ìƒì„±ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ìƒì„± ì „ìš©ì…ë‹ˆë‹¤.');
  }

  async generateImage(prompt: string, options?: { quality?: 'low' | 'medium' | 'high'; size?: '512x768' | '768x512' | '1024x1024' | '1024x1536' | '1536x1024' }): Promise<string> {
    // Runware êµ¬í˜„ì€ ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ ì—ëŸ¬ ì²˜ë¦¬
    throw new Error('Runware ì´ë¯¸ì§€ ìƒì„±ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
  }
}

export class LLMClientFactory {
  private static writingClient: BaseLLMClient | null = null;
  private static imageClient: BaseLLMClient | null = null;

  static createClient(config: LLMConfig): BaseLLMClient {
    switch (config.provider) {
      case 'openai':
        return new OpenAIClient(config);
      case 'claude':
        return new ClaudeClient(config);
      case 'gemini':
        return new GeminiClient(config);
      case 'runware':
        return new RunwareClient(config);
      default:
        throw new Error(`ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM ê³µê¸‰ì—…ì²´: ${config.provider}`);
    }
  }

  static setWritingClient(config: LLMConfig): void {
    this.writingClient = this.createClient(config);
  }

  static setImageClient(config: LLMConfig): void {
    this.imageClient = this.createClient(config);
  }

  static getWritingClient(): BaseLLMClient {
    if (!this.writingClient) {
      throw new Error('Writing LLM client not configured');
    }
    return this.writingClient;
  }

  static getImageClient(): BaseLLMClient {
    if (!this.imageClient) {
      throw new Error('Image LLM client not configured');
    }
    return this.imageClient;
  }

  static hasWritingClient(): boolean {
    return this.writingClient !== null;
  }

  static hasImageClient(): boolean {
    return this.imageClient !== null;
  }
}